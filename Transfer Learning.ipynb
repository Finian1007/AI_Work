{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Layers for FNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# Layers for CNN\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# For data preprocessing\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Collection and Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fashion MNIST dataset\n",
    "(x_train, y_train_rawdata), (x_test, y_test_rawdata) = datasets.fashion_mnist.load_data()\n",
    "\n",
    "# 標準化\n",
    "x_train = x_train / x_train.max()\n",
    "x_test = x_test / x_test.max()\n",
    "\n",
    "# One-hot encoding\n",
    "y_train = to_categorical(y_train_rawdata, 10)\n",
    "y_test = to_categorical(y_test_rawdata, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10070\n",
    "plt.imshow(x_train[n])\n",
    "plt.title(class_names[y_train_rawdata[n].squeeze()])\n",
    "plt.axis(\"off\");\n",
    "print(x_train[n].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape成(28, 28, 1)以方便進模型\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 建立用於分類 Fashion MNIST 的CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_layers = [Conv2D(32, (5, 5), input_shape=(28, 28, 1), padding=\"same\", activation=\"relu\", name=\"Conv2D_1\"),\n",
    "              MaxPool2D(),\n",
    "              Conv2D(64, (5, 5), padding=\"same\", activation=\"relu\", name='Conv2D_2'),\n",
    "              MaxPool2D(),\n",
    "              Conv2D(128, (5, 5), padding=\"same\", activation=\"relu\", name='Conv2D_3'),\n",
    "              GlobalAveragePooling2D()]\n",
    "\n",
    "FC_layers = [Dense(units=87, activation=\"relu\"),\n",
    "             Dense(units=10, activation='softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fashion = Sequential(CNN_layers + FC_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv2D_1 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 87)                11223     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                880       \n",
      "=================================================================\n",
      "Total params: 269,127\n",
      "Trainable params: 269,127\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_fashion.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fashion.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr = 0.05), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 95s 2ms/sample - loss: 1.2272 - accuracy: 0.5248 - val_loss: 0.8168 - val_accuracy: 0.6814\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 93s 2ms/sample - loss: 0.6990 - accuracy: 0.7289 - val_loss: 0.7661 - val_accuracy: 0.7120\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 94s 2ms/sample - loss: 0.5986 - accuracy: 0.7708 - val_loss: 0.5900 - val_accuracy: 0.7816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15d68d050>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fashion.fit(x_train, y_train, batch_size=64, epochs=3, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 23s 377us/sample - loss: 0.5661 - accuracy: 0.7883\n",
      "10000/10000 [==============================] - 4s 379us/sample - loss: 0.5900 - accuracy: 0.7816\n",
      "Train Accuracy: 78.8266658782959\n",
      "Test Accuracy: 78.15999984741211\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "score_train = model_fashion.evaluate(x_train, y_train)\n",
    "score_test = model_fashion.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Train Accuracy: {score_train[1]*100}')\n",
    "print(f'Test Accuracy: {score_test[1]*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 利用 Transfer Learning 中的Layer Transfer技巧，訓練用於分類MNIST資料集之CNN Model。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = range(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST\n",
    "(u_train, v_train_rawdata), (u_test, v_test_rawdata) = datasets.mnist.load_data()\n",
    "\n",
    "# 標準化\n",
    "u_train = u_train / u_train.max()\n",
    "u_test = u_test / u_test.max()\n",
    "\n",
    "# One-hot encoding\n",
    "v_train = to_categorical(v_train_rawdata, 10)\n",
    "v_test = to_categorical(v_test_rawdata, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10070\n",
    "plt.imshow(u_train[n])\n",
    "plt.title(name_list[v_train_rawdata[n].squeeze()])\n",
    "plt.axis(\"off\");\n",
    "print(u_train[n].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape成(28, 28, 1)以方便進模型\n",
    "u_train = u_train.reshape(60000, 28, 28, 1)\n",
    "u_test = u_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 跟上面的模型借CNN_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_layers_mnist = [Dense(units=87, activation='relu'),\n",
    "                   Dense(units=77, activation='relu'),\n",
    "                   Dense(units=10, activation='softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv2D_1 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 87)                11223     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 77)                6776      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                780       \n",
      "=================================================================\n",
      "Total params: 275,803\n",
      "Trainable params: 275,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mnist = Sequential(CNN_layers+FC_layers_mnist)\n",
    "model_mnist.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 採Frozen方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in CNN_layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv2D_1 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 87)                11223     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 77)                6776      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                780       \n",
      "=================================================================\n",
      "Total params: 275,803\n",
      "Trainable params: 18,779\n",
      "Non-trainable params: 257,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mnist.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 15s 245us/sample - loss: 0.9504 - accuracy: 0.7035 - val_loss: 0.5162 - val_accuracy: 0.8421\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 14s 240us/sample - loss: 0.4537 - accuracy: 0.8615 - val_loss: 0.3696 - val_accuracy: 0.8863\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 15s 250us/sample - loss: 0.3568 - accuracy: 0.8904 - val_loss: 0.3214 - val_accuracy: 0.9034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15d666b10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mnist.compile(loss='categorical_crossentropy', \n",
    "                    optimizer=Adam(),\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "model_mnist.fit(u_train, v_train,\n",
    "                batch_size=128, \n",
    "                epochs=3,\n",
    "                validation_data=(u_test, v_test)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結論：從分類fashion_mnist的神經網路那邊借CNN層，拿來訓練分類MNIST資料集的模型，除了在訓練速度上省下不少時間以外，最後準確度也有非常好的表現！因此日後如果在訓練模型時，可以試著從自身訓練過的模型想看看有何可借之處，就不用每次都從頭訓練了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
